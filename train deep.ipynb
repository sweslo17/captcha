{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import L1L2,l2, l1\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "#from keras.utils.layer_utils import layer_from_config\n",
    "from keras.layers import deserialize as layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from utils import preprocess_bw\n",
    "from sklearn import svm\n",
    "import skimage\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path,label)):\n",
    "            for filename in os.listdir(os.path.join(path,label)):\n",
    "                if not filename.startswith(\".\"):\n",
    "                    img = cv2.imread(os.path.join(path,label,filename), 0)\n",
    "                    #preprocessed = preprocess_bw(img)\n",
    "                    output = skimage.transform.resize(img, (20,20), mode=\"reflect\",preserve_range=True)\n",
    "                    X.append(output)\n",
    "                    y.append(int(label))\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(\"training2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "X_test = np.expand_dims(X_test,1)\n",
    "X_train = np.expand_dims(X_train,1)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5760, 1, 20, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "def norm_input(x): return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_5 (Lambda)            (None, 1, 20, 20)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 18, 18)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32, 18, 18)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 16, 16)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 8, 8)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 6, 6)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64, 6, 6)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 64, 4, 4)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 64, 2, 2)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 205,290\n",
      "Trainable params: 203,498\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Lambda(norm_input, input_shape=(1,20,20)),\n",
    "    Convolution2D(32,3,3, activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(32,3,3, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(64,3,3, activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(64,3,3, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model):\n",
    "    model.fit_generator(batches, batches.n, nb_epoch=1, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.n)\n",
    "    model.optimizer.lr=0.1\n",
    "    model.fit_generator(batches, batches.n, nb_epoch=4, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.n)\n",
    "    model.optimizer.lr=0.01\n",
    "    model.fit_generator(batches, batches.n, nb_epoch=12, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.n)\n",
    "    model.optimizer.lr=0.001\n",
    "    model.fit_generator(batches, batches.n, nb_epoch=18, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.n)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow(X_train, y_train, batch_size=64)\n",
    "test_batches = gen.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='model/weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., 5760, verbose=1, validation_steps=1440, epochs=10, validation_data=<keras.pre..., callbacks=[<keras.ca...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5760 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9875Epoch 00000: saving model to model/weights.00-0.02.hdf5\n",
      "5760/5760 [==============================] - 145s - loss: 0.0405 - acc: 0.9875 - val_loss: 0.0173 - val_acc: 0.9965\n",
      "Epoch 2/10\n",
      "5757/5760 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9987Epoch 00001: saving model to model/weights.01-0.00.hdf5\n",
      "5760/5760 [==============================] - 146s - loss: 0.0043 - acc: 0.9987 - val_loss: 4.6812e-05 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "5757/5760 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993Epoch 00002: saving model to model/weights.02-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0046 - val_acc: 0.9979\n",
      "Epoch 4/10\n",
      "5757/5760 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995Epoch 00003: saving model to model/weights.03-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0047 - val_acc: 0.9986\n",
      "Epoch 5/10\n",
      "5757/5760 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996Epoch 00004: saving model to model/weights.04-0.02.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0214 - val_acc: 0.9951\n",
      "Epoch 6/10\n",
      "5759/5760 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996Epoch 00005: saving model to model/weights.05-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0020 - val_acc: 0.9986\n",
      "Epoch 7/10\n",
      "5758/5760 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9997Epoch 00006: saving model to model/weights.06-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0010 - acc: 0.9997 - val_loss: 7.7623e-06 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "5757/5760 [============================>.] - ETA: 0s - loss: 7.8988e-04 - acc: 0.9998Epoch 00007: saving model to model/weights.07-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 7.8947e-04 - acc: 0.9998 - val_loss: 2.0315e-05 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "5758/5760 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998Epoch 00008: saving model to model/weights.08-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0010 - acc: 0.9998 - val_loss: 1.9272e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "5759/5760 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998Epoch 00009: saving model to model/weights.09-0.00.hdf5\n",
      "5760/5760 [==============================] - 147s - loss: 0.0011 - acc: 0.9998 - val_loss: 5.1361e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9efafac208>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.n, nb_epoch=10, verbose=1,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.n, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = model.predict(X_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8, 2, 7, ..., 0, 8, 8]), array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=-1), np.max(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
